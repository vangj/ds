{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression with dummy variables, no tears\n",
    "\n",
    "This notebook shows how you can apply ordinary least square (OLS) regression to a dataset with both continuous and categorical data. OLS regression only operates on continuous variables, and one way to allow categorical variables to be a part of the model is to create dummy variables out of the categorical ones. One way to create dummy variables out of a categorical variable is with [one-hot encoding](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) (OHE). \n",
    "\n",
    "The idea behind OHE is very simple. Imagine we have a categorical variable `gender` and it has only two values `male` and `female` (we know that there are [more genders](https://en.wikipedia.org/wiki/Third_gender) than male and female, but, bear with and forgive me, this example is only for illustration purposes). We might observe many instances of gender values as in the following table.\n",
    "\n",
    "<table>\n",
    "    <tr><th>gender</th></tr>\n",
    "    <tr><td>male</td></tr>\n",
    "    <tr><td>female</td></tr>\n",
    "    <tr><td>female</td></tr>\n",
    "    <tr><td>male</td></tr>\n",
    "</table>\n",
    "\n",
    "If we apply OHE to this gender variable, we will get the following data.\n",
    "\n",
    "<table>\n",
    "    <tr><th>gender_male</th><th>gender_female</th></tr>\n",
    "    <tr><td>1</td><td>0</td></tr>\n",
    "    <tr><td>0</td><td>1</td></tr>\n",
    "    <tr><td>0</td><td>1</td></tr>\n",
    "    <tr><td>1</td><td>0</td></tr>\n",
    "</table>\n",
    "\n",
    "Here's another example; imagine we have a categorical variable `height` and it has three values `tall`, `medium`, and `short`. We might observe many instances of height values as in the following table.\n",
    "\n",
    "<table>\n",
    "    <tr><th>height</th></tr>\n",
    "    <tr><td>tall</td></tr>\n",
    "    <tr><td>tall</td></tr>\n",
    "    <tr><td>short</td></tr>\n",
    "    <tr><td>medium</td></tr>\n",
    "</table>\n",
    "\n",
    "Applying OHE to this height variable, we will get the following data.\n",
    "\n",
    "<table>\n",
    "    <tr><th>height_tall</th><th>height_medium</th><th>height_short</th></tr>\n",
    "    <tr><td>1</td><td>0</td><td>0</td></tr>\n",
    "    <tr><td>1</td><td>0</td><td>0</td></tr>\n",
    "    <tr><td>0</td><td>0</td><td>1</td></tr>\n",
    "    <tr><td>0</td><td>1</td><td>0</td></tr>\n",
    "</table>\n",
    "\n",
    "Generally speaking, for a categorical variable with $k$ values, we will have $k$ new variables after applying OHE.\n",
    "\n",
    "Now, let us say we want to apply OLS regression on three independent variables \n",
    "\n",
    "* `age` (continuous), \n",
    "* `gender` (categorical), and\n",
    "* `height` (categorical).\n",
    "\n",
    "Our output variable $Y$ is a continuous dependent variable and could represent something like blood pressure. Then, our OLS regression model looks like the following. \n",
    "\n",
    "$Y = 5.0 + w_1 A + w_2 G + w_3 H$\n",
    "\n",
    "where\n",
    "\n",
    "* $A$ is age\n",
    "* $G$ is gender\n",
    "* $H$ is height\n",
    "\n",
    "We know that we cannot use the categorical values directly in the OLS regression model, and so we OHE each of the categorical variables. After applying OHE to the categorical variables, our OLS regression model looks like the following.\n",
    "\n",
    "$Y = 5.0 + w_1 A + w_2 G_{\\mathrm{male}} + w_3 G_{\\mathrm{female}} + w_4 H_{\\mathrm{short}} + w_5 H_{\\mathrm{medium}} + w_6 H_{\\mathrm{tall}}$\n",
    "\n",
    "Of course, we can try to fit this model, but modeling the data in this way results in a few problems.\n",
    "\n",
    "* [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity)\n",
    "* [dummy variable trap](https://en.wikipedia.org/wiki/Dummy_variable_%28statistics%29)\n",
    "\n",
    "Multicollinearity is when an independent variable can be predicted linearly from others. It presents coefficient estimation and interpretation problems. The dummy variable trap is when we use all the OHE variables in the model and the system of equations approach (matrix algebra) to estimating the coefficients will not have a unique solution. \n",
    "\n",
    "The way to deal with this situation is to remove one OHE variable from the set associated with a categorical variable. Which one do we remove? Here's two approaches.\n",
    "\n",
    "* Remove the OHE variable whose associated value is the most frequent.\n",
    "* Remove the OHE variable whose associated value is the most natural reference point.\n",
    "\n",
    "In this toy example, we might remove $G_{\\mathrm{female}}$ since it is the most frequently observed value, and we might remove $H_{\\mathrm{short}}$ since it is a natural reference point. Our new OLS regression model will look like the following.\n",
    "\n",
    "$Y = 5.0 + w_1 A + w_2 G_{\\mathrm{male}} + w_3 H_{\\mathrm{tall}} + w_4 H_{\\mathrm{medium}}$\n",
    "\n",
    "Note that $w_2, w_3, w_4$ are called __differential intercept coefficients__ and interpreted as follows\n",
    "\n",
    "* $w_2$ is the difference in $y$ between males and females holding everything else constant\n",
    "* $w_3$ is the difference in $y$ between tall and short holding everything else constant\n",
    "* $w_4$ is the difference in $y$ between medium and short holding everything else constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample (synthetic) data\n",
    "\n",
    "\n",
    "\n",
    "We will sample $A$ and $\\epsilon$ (error) from normal distributions and $G$ and $H$ from multinomial distributions as follows.\n",
    "\n",
    "* $A \\sim \\mathcal{N}(25, 1)$\n",
    "* $G \\sim \\mathrm{Mult}([p_{\\mathrm{male}}, p_{\\mathrm{female}}]) = \\mathrm{Mult}([0.2, 0.8])$\n",
    "* $H \\sim \\mathrm{Mult}([p_{\\mathrm{short}}, p_{\\mathrm{medium}}, p_{\\mathrm{tall}}]) = \\mathrm{Mult}([0.2, 0.6, 0.2])$\n",
    "* $\\epsilon \\sim \\mathcal{N}(0, 1)$\n",
    "\n",
    "Then, we will simulate $Y$ according to the following equation.\n",
    "\n",
    "$Y = 5.0 + 3 A + 0.8 G_{\\mathrm{male}} - 2 H_{\\mathrm{tall}} + 3 H_{\\mathrm{medium}} + \\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T01:31:33.133243Z",
     "start_time": "2018-05-07T01:31:32.544630Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from scipy.stats import multinomial, dirichlet, norm\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(37)\n",
    "n = 2000\n",
    "half = n / 2\n",
    "\n",
    "intercept = np.ones(n).reshape(n, 1)\n",
    "age = norm.rvs(25, 1, size=n).reshape(n, 1)\n",
    "gender = multinomial.rvs(1, [0.2, 0.8], size=n)\n",
    "height = multinomial.rvs(1, [0.2, 0.6, 0.2], size=n)\n",
    "noise = norm.rvs(0, 1, size=n)\n",
    "\n",
    "# the full synthetic data\n",
    "X = np.hstack([intercept, age, gender, height]) # all the variables\n",
    "Z = np.delete(X, [2, 4], axis=1) # get rid of 2 dummy variables\n",
    "w = np.array([5.0, 3.0, 0.0, 0.8, 0.0, -2.0, 3.0], dtype=np.float)\n",
    "y = np.dot(X, w) + noise\n",
    "\n",
    "# split the data into half training and half testing\n",
    "X_train = X[0:half, :]\n",
    "Z_train = Z[0:half, :]\n",
    "y_train = y[0:half]\n",
    "\n",
    "X_test = X[half:n, :]\n",
    "Z_test = Z[half:n, :]\n",
    "y_test = y[half:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn from the data with all variables\n",
    "\n",
    "We will use OLS, Lasso, and Ridge regression to learn the weights on the data with all the variables (no dummy variable removed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T01:31:33.317161Z",
     "start_time": "2018-05-07T01:31:33.139135Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model coefficients\n",
      "3.14,3.00,1.20,1.94,0.71,-1.33,3.76, shape=1x7, OLS\n",
      "0.00,3.20,-0.00,0.00,0.00,-0.00,0.00, shape=1x7, LASSO\n",
      "2.34,3.06,0.80,1.54,0.44,-1.58,3.48, shape=1x7, RIDGE\n",
      "\n",
      "R^2\n",
      "0.92537 OLS\n",
      "0.64030 LASSO\n",
      "0.92519 RIDGE\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def print_mat(m):\n",
    "    rows = m.shape[0]\n",
    "    cols = m.shape[1]\n",
    "    for r in range(rows):\n",
    "        print(','.join(['{:.2f}'.format(m[r, c]) for c in range(cols)]))\n",
    "    print('{}x{}'.format(rows, cols))\n",
    "    \n",
    "def print_vec(hint, v):\n",
    "    print(\n",
    "        ','.join(['{:.2f}'.format(c) for c in v]) + \n",
    "        ', shape=' + '1x{}'.format(len(v)) +\n",
    "        ', ' + hint\n",
    "    ) \n",
    "    \n",
    "lr = LinearRegression(fit_intercept=False)\n",
    "la = Lasso(fit_intercept=False)\n",
    "ri = Ridge(fit_intercept=False)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "la.fit(X_train, y_train)\n",
    "ri.fit(X_train, y_train)\n",
    "\n",
    "print('model coefficients')\n",
    "print_vec('OLS', lr.coef_)\n",
    "print_vec('LASSO', la.coef_)\n",
    "print_vec('RIDGE', ri.coef_)\n",
    "\n",
    "print('')\n",
    "print('R^2')\n",
    "print('{:.5f} OLS'.format(r2_score(y_test, lr.predict(X_test))))\n",
    "print('{:.5f} LASSO'.format(r2_score(y_test, la.predict(X_test))))\n",
    "print('{:.5f} RIDGE'.format(r2_score(y_test, ri.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn from the data with 2 dummy variables removed\n",
    "\n",
    "We will use OLS, Lasso, and Ridge regression to learn the weights on the data with 2 dummy variables removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T01:31:33.343330Z",
     "start_time": "2018-05-07T01:31:33.322635Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model coefficients\n",
      "5.04,3.00,0.74,-2.04,3.05, shape=1x5, OLS\n",
      "0.00,3.20,0.00,-0.00,0.00, shape=1x5, LASSO\n",
      "3.09,3.08,0.74,-2.02,3.05, shape=1x5, RIDGE\n",
      "\n",
      "R^2\n",
      "0.92537 OLS\n",
      "0.64030 LASSO\n",
      "0.92499 RIDGE\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(fit_intercept=False)\n",
    "la = Lasso(fit_intercept=False)\n",
    "ri = Ridge(fit_intercept=False)\n",
    "\n",
    "lr.fit(Z_train, y_train)\n",
    "la.fit(Z_train, y_train)\n",
    "ri.fit(Z_train, y_train)\n",
    "\n",
    "print('model coefficients')\n",
    "print_vec('OLS', lr.coef_)\n",
    "print_vec('LASSO', la.coef_)\n",
    "print_vec('RIDGE', ri.coef_)\n",
    "\n",
    "print('')\n",
    "print('R^2')\n",
    "print('{:.5f} OLS'.format(r2_score(y_test, lr.predict(Z_test))))\n",
    "print('{:.5f} LASSO'.format(r2_score(y_test, la.predict(Z_test))))\n",
    "print('{:.5f} RIDGE'.format(r2_score(y_test, ri.predict(Z_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing models without and with removing dummy variables\n",
    "\n",
    "Note how LASSO regression's $R^2$ values are dismal without and with removing the dummy variables. LASSO tends to think that only age matters in predicting the outcome.\n",
    "\n",
    "Note how OLS and RIDGE regressions' $R^2$ values are really good without and with removing the dummy variables. When removing the dummy variables, the coefficients between OLS and RIDGE are nearly identical, however, the intercept learned from OLS matches the true value.\n",
    "\n",
    "Since the OLS and RIDGE regression models learned from removing the dummy variables predict unseen data equally as well as their corresponding models without removing the variables, we should favor them (parsimony and [Occam's razor](https://simple.wikipedia.org/wiki/Occam%27s_razor))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take a Look!\n",
    "\n",
    "Take a look at [Dr. Vladimir Vapnik](https://en.wikipedia.org/wiki/Vladimir_Vapnik)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
